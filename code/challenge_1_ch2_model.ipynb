{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 5386 preprocessed EEG files (task: contrastChangeDetection)\n",
      "ðŸ”— Matched 5386 CCD EEGâ†”event pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse CCD events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5386/5386 [01:35<00:00, 56.21it/s] \n",
      "Verify cache: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 341/341 [00:09<00:00, 36.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CCD samples: 341 | RT mean=2863.3 ms, std=168.6 ms, tertiles=(2870,2940)\n",
      "[INFO] Loaded CH2 weights from /home/RA/EEG_Challenge/Challenge2/best_cbramod_cached_finetune.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep01] train=1.3907 | val MAE=0.4946 MSE=1.0774 NRMSE=0.9999 R2=0.000\n",
      "  âœ… Saved checkpoint: cbramod_ssl_ctx_react_aux_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep02] train=1.3861 | val MAE=0.4739 MSE=1.0866 NRMSE=1.0042 R2=-0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep03] train=1.3624 | val MAE=0.4857 MSE=1.0856 NRMSE=1.0038 R2=-0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep04] train=1.3127 | val MAE=0.4917 MSE=1.0928 NRMSE=1.0071 R2=-0.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep05] train=1.3413 | val MAE=0.4816 MSE=1.0965 NRMSE=1.0088 R2=-0.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep06] train=1.3863 | val MAE=0.4799 MSE=1.0968 NRMSE=1.0089 R2=-0.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep07] train=1.3763 | val MAE=0.5028 MSE=1.1051 NRMSE=1.0128 R2=-0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep08] train=1.3669 | val MAE=0.4989 MSE=1.1047 NRMSE=1.0125 R2=-0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep09] train=1.3232 | val MAE=0.4967 MSE=1.1035 NRMSE=1.0120 R2=-0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep10] train=1.3744 | val MAE=0.4932 MSE=1.0987 NRMSE=1.0098 R2=-0.020\n",
      "Best: {'MAE': 0.49458539485931396, 'MSE': 1.0773568153381348, 'NRMSE': 0.9999401571923082, 'R2': 0.00011962652206420898, 'epoch': 1}\n",
      "Saved: cbramod_ssl_ctx_react_aux_last.pth\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CBraMod (from CH2) â†’ CCD RT downstream (CH1) with\n",
    "# temporal context, reaction-phase weighting, norm alignment,\n",
    "# and multi-task auxiliary (RT tertile classification).\n",
    "# Requires: 100 Hz .npy cache for EEG (.set -> *_cached.npy)\n",
    "# ============================================================\n",
    "\n",
    "import os, random, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------\n",
    "# Config\n",
    "# ------------------\n",
    "BIDS_ROOT         = \"/data5/open_data/HBN/EEG_BIDS/\"\n",
    "PREPROCESSED_ROOT = \"/data5/open_data/HBN/Preprocessed_EEG/0922try_bySubject/\"\n",
    "CACHE_DIR         = \"/data5/open_data/HBN/cache_eeg_100hz_noref\"   # where *_cached.npy lives\n",
    "CH2_CKPT_PATH     = \"/home/RA/EEG_Challenge/Challenge2/best_cbramod_cached_finetune.pth\"  # âœ… your CH2 weights\n",
    "\n",
    "TARGET_SFREQ = 100\n",
    "BATCH_SIZE   = 64\n",
    "NUM_WORKERS  = 4\n",
    "EPOCHS       = 10\n",
    "LR           = 1e-3\n",
    "WD           = 1e-4\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED         = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Temporal context (seconds): pre + capped reaction + post\n",
    "PRE_CTX_S     = 1.0\n",
    "REACT_CAP_S   = 1.0   # include up to 1s of reaction (onsetâ†’press)\n",
    "POST_CTX_S    = 0.5\n",
    "TOTAL_LEN_S   = PRE_CTX_S + REACT_CAP_S + POST_CTX_S  # = 2.5s\n",
    "TOTAL_SAMPLES = int(TOTAL_LEN_S * TARGET_SFREQ)       # 250\n",
    "\n",
    "# Weighting for reaction length in the included segment\n",
    "WEIGHT_GAMMA  = 1.0   # w = 1 + GAMMA * (min(rt, REACT_CAP_S)/REACT_CAP_S)\n",
    "\n",
    "# Multi-task auxiliary\n",
    "AUX_NUM_CLASSES = 3   # fast / medium / slow tertiles\n",
    "AUX_LAMBDA      = 0.2\n",
    "\n",
    "# ============================================================\n",
    "# 1) Files and events\n",
    "# ============================================================\n",
    "def collect_preprocessed_files(root_path, task_name=None):\n",
    "    results = []\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        if \"sub-\" not in dirpath:\n",
    "            continue\n",
    "        for fn in filenames:\n",
    "            low = fn.lower()\n",
    "            if not low.endswith(\"_eeg_pp.set\"):\n",
    "                continue\n",
    "            if task_name and (task_name.lower() not in low):\n",
    "                continue\n",
    "            results.append(os.path.join(dirpath, fn))\n",
    "    results = sorted(results)\n",
    "    print(f\"[INFO] Found {len(results)} preprocessed EEG files ({'task: '+task_name if task_name else 'all'})\")\n",
    "    return [(f, \"\") for f in results]\n",
    "\n",
    "def collect_ccd_event_files(bids_root):\n",
    "    ev_files = glob(os.path.join(\n",
    "        bids_root, \"ds*/sub-*\", \"eeg\", \"sub-*_task-contrastChangeDetection*_events.tsv\"\n",
    "    ))\n",
    "    return ev_files\n",
    "\n",
    "def match_eeg_to_event(preproc_files, bids_root):\n",
    "    ev_files = collect_ccd_event_files(bids_root)\n",
    "    ev_dict = {os.path.basename(ef).replace(\"_events.tsv\", \"\"): ef for ef in ev_files}\n",
    "    pairs = []\n",
    "    for eeg_path, _ in preproc_files:\n",
    "        base = os.path.basename(eeg_path)\n",
    "        key = base.replace(\"_eeg_pp.set\", \"\").replace(\".set\", \"\")\n",
    "        if key in ev_dict:\n",
    "            pairs.append((eeg_path, ev_dict[key]))\n",
    "        else:\n",
    "            key_no_run = key.replace(\"_run-1\",\"\").replace(\"_run-2\",\"\")\n",
    "            matched = None\n",
    "            for k,v in ev_dict.items():\n",
    "                k_norm = k.replace(\"_run-1\",\"\").replace(\"_run-2\",\"\")\n",
    "                if k_norm == key_no_run:\n",
    "                    matched = v; break\n",
    "            if matched:\n",
    "                pairs.append((eeg_path, matched))\n",
    "    print(f\"ðŸ”— Matched {len(pairs)} CCD EEGâ†”event pairs.\")\n",
    "    return pairs\n",
    "\n",
    "# ============================================================\n",
    "# 2) Cache + window\n",
    "# ============================================================\n",
    "def cached_load_eeg(eeg_set_path):\n",
    "    fname = os.path.basename(eeg_set_path).replace(\".set\", \"_cached.npy\")\n",
    "    path  = os.path.join(CACHE_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        return np.load(path)  # (C, T)\n",
    "    return None\n",
    "\n",
    "def make_context_window(x_ct, onset_s, rt_ms,\n",
    "                        pre_s=PRE_CTX_S, react_cap_s=REACT_CAP_S, post_s=POST_CTX_S,\n",
    "                        sfreq=TARGET_SFREQ):\n",
    "    \"\"\"\n",
    "    Extract [onset - pre, onset + min(rt,cap) + post] as a fixed length segment (pad if needed).\n",
    "    Returns seg(C, TOTAL_SAMPLES), react_fracâˆˆ[0,1].\n",
    "    \"\"\"\n",
    "    Ttot = int((pre_s + react_cap_s + post_s) * sfreq)\n",
    "    if x_ct is None:\n",
    "        return np.zeros((128, Ttot), np.float32), 0.0\n",
    "\n",
    "    onset = int(onset_s * sfreq)\n",
    "    react_len = int(min(rt_ms/1000.0, react_cap_s) * sfreq)\n",
    "    t0 = max(0, onset - int(pre_s * sfreq))\n",
    "    t1_desired = onset + int(react_cap_s * sfreq) + int(post_s * sfreq)\n",
    "    seg = x_ct[:, t0:t1_desired]\n",
    "    need = Ttot - seg.shape[1]\n",
    "    if need > 0:\n",
    "        seg = np.pad(seg, ((0,0),(0,need)), mode=\"constant\")\n",
    "    else:\n",
    "        seg = seg[:, :Ttot]\n",
    "\n",
    "    react_frac = (react_len / int(react_cap_s * sfreq)) if react_cap_s > 0 else 0.0\n",
    "    react_frac = float(np.clip(react_frac, 0.0, 1.0))\n",
    "    return seg.astype(np.float32), react_frac\n",
    "\n",
    "# ============================================================\n",
    "# 3) Parse trials\n",
    "# ============================================================\n",
    "def extract_ccd_trials(df):\n",
    "    if df is None or df.empty or \"onset\" not in df.columns or \"value\" not in df.columns:\n",
    "        return []\n",
    "    trials = []\n",
    "    on  = df[\"onset\"].astype(float).values\n",
    "    val = df[\"value\"].astype(str).values\n",
    "    fb  = df[\"feedback\"].astype(str).values if \"feedback\" in df.columns else [\"n/a\"] * len(df)\n",
    "    starts  = [i for i,v in enumerate(val) if \"contrastTrial_start\" in v]\n",
    "    presses = [i for i,v in enumerate(val) if \"buttonPress\" in v]\n",
    "    for ti in starts:\n",
    "        t0 = on[ti]\n",
    "        later = [pi for pi in presses if on[pi] > t0]\n",
    "        if not later: \n",
    "            continue\n",
    "        pi = later[0]\n",
    "        rt = (on[pi]-t0) * 1000.0\n",
    "        if 100 <= rt <= 3000 and \"smiley\" in fb[pi].lower():\n",
    "            trials.append((t0, rt))\n",
    "    return trials\n",
    "\n",
    "# ============================================================\n",
    "# 4) Dataset (context window + weighting + tertile class)\n",
    "# ============================================================\n",
    "class CCDContextDataset(Dataset):\n",
    "    def __init__(self, eeg_event_pairs):\n",
    "        raw_samples = []\n",
    "        for eeg_path, ev_path in tqdm(eeg_event_pairs, desc=\"Parse CCD events\"):\n",
    "            if not os.path.exists(ev_path): \n",
    "                continue\n",
    "            df = pd.read_csv(ev_path, sep=\"\\t\")\n",
    "            for onset, rt in extract_ccd_trials(df):\n",
    "                raw_samples.append((eeg_path, onset, rt))\n",
    "\n",
    "        # keep only samples with cache present\n",
    "        self.samples = []\n",
    "        for eeg_path, onset, rt in tqdm(raw_samples, desc=\"Verify cache\"):\n",
    "            if cached_load_eeg(eeg_path) is not None:\n",
    "                self.samples.append((eeg_path, onset, float(rt)))\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(\"No valid CCD samples with cache.\")\n",
    "\n",
    "        # stats for z-score RT label and tertiles\n",
    "        rts = np.array([rt for _,_,rt in self.samples], np.float32)\n",
    "        self.rt_mean = float(rts.mean())\n",
    "        self.rt_std  = float(rts.std() + 1e-6)\n",
    "        self.t1, self.t2 = np.percentile(rts, [33.3, 66.6]).tolist()\n",
    "        print(f\"âœ… CCD samples: {len(self.samples)} | RT mean={self.rt_mean:.1f} ms, std={self.rt_std:.1f} ms, tertiles=({self.t1:.0f},{self.t2:.0f})\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg_path, onset_s, rt_ms = self.samples[idx]\n",
    "        X = cached_load_eeg(eeg_path)  # (C,T), z-scored already\n",
    "        seg, react_frac = make_context_window(X, onset_s, rt_ms)\n",
    "\n",
    "        # labels\n",
    "        rt_norm = (rt_ms - self.rt_mean) / self.rt_std\n",
    "        # tertile class\n",
    "        if rt_ms < self.t1: cls = 0\n",
    "        elif rt_ms < self.t2: cls = 1\n",
    "        else: cls = 2\n",
    "\n",
    "        weight = 1.0 + WEIGHT_GAMMA * react_frac\n",
    "        return (\n",
    "            torch.from_numpy(seg),                    # (C, Ttot)\n",
    "            torch.tensor([rt_norm], dtype=torch.float32),  # (1,)\n",
    "            torch.tensor(cls, dtype=torch.long),      # ()\n",
    "            torch.tensor([weight], dtype=torch.float32)    # (1,)\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# 5) CBraMod encoder (same as CH2) + context/normalization\n",
    "# ============================================================\n",
    "class ChannelAffine(nn.Module):\n",
    "    \"\"\"Learnable per-channel scale/bias (alignment).\"\"\"\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C))\n",
    "        self.beta  = nn.Parameter(torch.zeros(C))\n",
    "    def forward(self, x):  # (B,C,T)\n",
    "        return x * self.gamma[:,None] + self.beta[:,None]\n",
    "\n",
    "class TemporalContextBlock(nn.Module):\n",
    "    \"\"\"Depthwise dilated conv pyramid to expand temporal RF before SincConv.\"\"\"\n",
    "    def __init__(self, C, dilations=(1,2,4), k=5):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(C, C, kernel_size=k, padding=d*(k//2), dilation=d, groups=C)\n",
    "            for d in dilations\n",
    "        ])\n",
    "        self.proj = nn.Conv1d(C, C, kernel_size=1)\n",
    "        self.act  = nn.ReLU()\n",
    "    def forward(self, x):  # (B,C,T)\n",
    "        y = 0\n",
    "        for conv in self.convs:\n",
    "            y = y + conv(x)\n",
    "        y = self.proj(self.act(y))\n",
    "        return x + y\n",
    "\n",
    "class SincConv1d(nn.Module):\n",
    "    def __init__(self, out_channels=64, kernel_size=129, sample_rate=100, min_hz=0.3, max_hz=45.0):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size  = kernel_size\n",
    "        self.sample_rate  = sample_rate\n",
    "        self.min_hz       = float(min_hz)\n",
    "        self.max_hz       = float(max_hz)\n",
    "        low  = torch.linspace(self.min_hz, self.max_hz - 5.0, out_channels)\n",
    "        band = torch.ones(out_channels) * 5.0\n",
    "        self.low_hz_  = nn.Parameter(low)\n",
    "        self.band_hz_ = nn.Parameter(band)\n",
    "        n = torch.arange(-(kernel_size // 2), kernel_size // 2 + 1).float()\n",
    "        self.register_buffer(\"n\", n)\n",
    "\n",
    "    def forward(self, x):  # x: (B, C, T)\n",
    "        B, C, T = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "        low  = torch.clamp(torch.abs(self.low_hz_), min=self.min_hz, max=self.max_hz - 1.0)\n",
    "        raw_high = low + torch.abs(self.band_hz_)\n",
    "        min_v, max_v = low + 1.0, torch.full_like(low, self.max_hz)\n",
    "        high = torch.clamp(raw_high, min=min_v, max=max_v)\n",
    "        n = self.n.to(device=device, dtype=dtype)\n",
    "        window = torch.hamming_window(self.kernel_size, periodic=False, dtype=dtype, device=device)\n",
    "        nyq = self.sample_rate / 2.0\n",
    "        filters = []\n",
    "        for i in range(self.out_channels):\n",
    "            f1, f2 = low[i]/nyq, high[i]/nyq\n",
    "            h1 = 2 * f2 * torch.sinc(2 * f2 * n)\n",
    "            h2 = 2 * f1 * torch.sinc(2 * f1 * n)\n",
    "            bandpass = (h1 - h2) * window\n",
    "            filters.append(bandpass)\n",
    "        filt = torch.stack(filters, dim=0).unsqueeze(1)  # (out, 1, K)\n",
    "        x_dw = x.view(B * C, 1, T)\n",
    "        y = F.conv1d(x_dw, filt, stride=1, padding=self.kernel_size // 2)\n",
    "        y = y.view(B, C, self.out_channels, y.shape[-1]).sum(dim=1)  # (B, out, T)\n",
    "        return y\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=8):\n",
    "        super().__init__()\n",
    "        self.fc1, self.fc2 = nn.Linear(c, c//r), nn.Linear(c//r, c)\n",
    "    def forward(self, x):\n",
    "        s = x.mean(-1)\n",
    "        e = torch.sigmoid(self.fc2(F.relu(self.fc1(s)))).unsqueeze(-1)\n",
    "        return x * e\n",
    "\n",
    "class CBraModBackbone(nn.Module):\n",
    "    def __init__(self, out_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(64,128,7,padding=3), nn.ReLU(),\n",
    "            nn.Conv1d(128,256,5,padding=2), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(256,out_dim)\n",
    "        self.out_dim = out_dim\n",
    "    def forward(self,x):\n",
    "        return self.fc(self.conv(x).squeeze(-1))\n",
    "\n",
    "class EncoderCBraMod(nn.Module):\n",
    "    \"\"\"Input alignment â†’ temporal context â†’ sinc â†’ SE â†’ backbone â†’ 512-d feature\"\"\"\n",
    "    def __init__(self, in_chans=128, sr=100, out_dim=512):\n",
    "        super().__init__()\n",
    "        self.affine = ChannelAffine(in_chans)\n",
    "        self.bn     = nn.BatchNorm1d(in_chans, affine=True)\n",
    "        self.ctx    = TemporalContextBlock(in_chans, dilations=(1,2,4), k=5)\n",
    "        self.front  = SincConv1d(out_channels=64, kernel_size=129, sample_rate=sr)\n",
    "        self.se     = SEBlock(64)\n",
    "        self.backbone = CBraModBackbone(out_dim=out_dim)\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, x):  # (B,C,T)\n",
    "        x = self.affine(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.ctx(x)\n",
    "        x = self.front(x)\n",
    "        x = self.se(x)\n",
    "        z = self.backbone(x)  # (B,512)\n",
    "        return z\n",
    "\n",
    "def load_ch2_weights_into_encoder(encoder: EncoderCBraMod, ckpt_path: str):\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"[WARN] CH2 ckpt not found: {ckpt_path}\")\n",
    "        return\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    # keys: \"front\", \"backbone\", maybe \"head\"\n",
    "    missing=[]\n",
    "    if \"front\" in sd: encoder.front.load_state_dict(sd[\"front\"], strict=False)\n",
    "    else: missing.append(\"front\")\n",
    "    if \"backbone\" in sd: encoder.backbone.load_state_dict(sd[\"backbone\"], strict=False)\n",
    "    else: missing.append(\"backbone\")\n",
    "    if missing:\n",
    "        print(f\"[WARN] Missing parts in ckpt: {missing}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Loaded CH2 weights from {ckpt_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Heads + loss\n",
    "# ============================================================\n",
    "class RtHead(nn.Module):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim, max(64, feat_dim//2)), nn.ReLU(),\n",
    "            nn.Linear(max(64, feat_dim//2), 1)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)  # (B,1)\n",
    "\n",
    "class AuxClsHead(nn.Module):\n",
    "    def __init__(self, feat_dim, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, max(64, feat_dim//2)), nn.ReLU(),\n",
    "            nn.Linear(max(64, feat_dim//2), n_classes)\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)  # (B,K)\n",
    "\n",
    "def weighted_mae(pred, target, weight):  # pred,target (B,1), weight (B,1)\n",
    "    return (weight * (pred - target).abs()).mean()\n",
    "\n",
    "# ============================================================\n",
    "# 7) Train / Eval\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(dl, encoder, rt_head, aux_head):\n",
    "    encoder.eval(); rt_head.eval(); aux_head.eval()\n",
    "    preds, trues = [], []\n",
    "    for x, y, cls, w in dl:\n",
    "        x = x.to(DEVICE).float()\n",
    "        y = y.to(DEVICE).float()\n",
    "        z = encoder(x)\n",
    "        yhat = rt_head(z)\n",
    "        preds.append(yhat.squeeze(1).cpu().numpy())\n",
    "        trues.append(y.squeeze(1).cpu().numpy())\n",
    "    preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "    mae = np.mean(np.abs(preds - trues))\n",
    "    mse = np.mean((preds - trues)**2)\n",
    "    nrmse = np.sqrt(mse) / (trues.std() + 1e-8)\n",
    "    r2 = 1 - np.sum((preds-trues)**2) / np.sum((trues-trues.mean())**2 + 1e-8)\n",
    "    return dict(MAE=float(mae), MSE=float(mse), NRMSE=float(nrmse), R2=float(r2))\n",
    "\n",
    "def train(train_dl, val_dl, encoder, rt_head, aux_head,\n",
    "          epochs=EPOCHS, lr=LR, wd=WD, aux_lambda=AUX_LAMBDA):\n",
    "    params = list(encoder.parameters()) + list(rt_head.parameters()) + list(aux_head.parameters())\n",
    "    opt = torch.optim.AdamW(params, lr=lr, weight_decay=wd)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "    best = {\"NRMSE\": 9e9, \"epoch\": -1}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        encoder.train(); rt_head.train(); aux_head.train()\n",
    "        losses=[]\n",
    "        for x, y, cls, w in tqdm(train_dl, desc=f\"[Train] ep{ep}/{epochs}\", leave=False):\n",
    "            x = x.to(DEVICE).float()\n",
    "            y = y.to(DEVICE).float()      # (B,1) normalized RT\n",
    "            cls = cls.to(DEVICE).long()   # (B,)\n",
    "            w = w.to(DEVICE).float()      # (B,1)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "                z = encoder(x)                 # (B,512)\n",
    "                yhat = rt_head(z)             # (B,1)\n",
    "                logits = aux_head(z)          # (B,K)\n",
    "                loss_reg = weighted_mae(yhat, y, w)\n",
    "                loss_aux = F.cross_entropy(logits, cls)\n",
    "                loss = loss_reg + aux_lambda * loss_aux\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        valm = evaluate(val_dl, encoder, rt_head, aux_head)\n",
    "        print(f\"[ep{ep:02d}] train={np.mean(losses):.4f} | \"\n",
    "              f\"val MAE={valm['MAE']:.4f} MSE={valm['MSE']:.4f} \"\n",
    "              f\"NRMSE={valm['NRMSE']:.4f} R2={valm['R2']:.3f}\")\n",
    "\n",
    "        if valm[\"NRMSE\"] < best[\"NRMSE\"]:\n",
    "            best = {**valm, \"epoch\": ep}\n",
    "            torch.save({\n",
    "                \"encoder\": encoder.state_dict(),\n",
    "                \"rt_head\": rt_head.state_dict(),\n",
    "                \"aux_head\": aux_head.state_dict(),\n",
    "                \"val_metrics\": valm\n",
    "            }, \"cbramod_ssl_ctx_react_aux_best.pth\")\n",
    "            with open(\"cbramod_ssl_ctx_react_aux_best.json\",\"w\") as f:\n",
    "                json.dump(best, f, indent=2)\n",
    "            print(\"  âœ… Saved checkpoint: cbramod_ssl_ctx_react_aux_best.pth\")\n",
    "    print(\"Best:\", best)\n",
    "\n",
    "# ============================================================\n",
    "# 8) Main\n",
    "# ============================================================\n",
    "def main():\n",
    "    # a) build CCD dataset from cached files\n",
    "    ccd_files = collect_preprocessed_files(PREPROCESSED_ROOT, task_name=\"contrastChangeDetection\")\n",
    "    pairs = match_eeg_to_event(ccd_files, BIDS_ROOT)\n",
    "    ds = CCDContextDataset(pairs)\n",
    "\n",
    "    # b) split train/val quickly (files randomized; for strict subject split replace with subj-based)\n",
    "    n_total = len(ds)\n",
    "    n_val = max(64, int(0.1*n_total))\n",
    "    n_tr  = n_total - n_val\n",
    "    train_ds, val_ds = random_split(ds, [n_tr, n_val], generator=torch.Generator().manual_seed(SEED))\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=max(1,NUM_WORKERS//2), pin_memory=True)\n",
    "\n",
    "    # c) model\n",
    "    encoder = EncoderCBraMod(in_chans=128, sr=TARGET_SFREQ, out_dim=512).to(DEVICE)\n",
    "    load_ch2_weights_into_encoder(encoder, CH2_CKPT_PATH)\n",
    "    rt_head  = RtHead(encoder.out_dim).to(DEVICE)\n",
    "    aux_head = AuxClsHead(encoder.out_dim, n_classes=AUX_NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "    # d) train\n",
    "    train(train_dl, val_dl, encoder, rt_head, aux_head, epochs=EPOCHS, lr=LR, wd=WD, aux_lambda=AUX_LAMBDA)\n",
    "\n",
    "    # e) export final (last state too)\n",
    "    torch.save({\n",
    "        \"encoder\": encoder.state_dict(),\n",
    "        \"rt_head\": rt_head.state_dict(),\n",
    "        \"aux_head\": aux_head.state_dict()\n",
    "    }, \"cbramod_ssl_ctx_react_aux_last.pth\")\n",
    "    print(\"Saved: cbramod_ssl_ctx_react_aux_last.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
